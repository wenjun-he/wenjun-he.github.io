---
layout: post
title:  ICC (Intraclass Correlation Coefficients) 
date: 2021-11-01
tags: markdown    
---

# A GUidleline of Selecting and Reporting Intraclass Correlation Coefficients
## Definitions of Different Types of Reliablity
![](/images/blog/ICC_definition.png)
## 10 Forms of ICC
![](/images/blog/10forms.png)
## How to select the right form of ICC
(1) Do we have the same set of raters for all subjects? 

(2) Do we have a sample of raters randomly selected from a larger population or a specific sample of raters? 

(3) Are we interested in the reliability of single rater or the mean value of multiple raters? 

(4) Do we concern about consistency or agreement?
### Model Selection
#### One-Way Random-Effects Model
Each subject is rated by a different set of raters who were randomly chosen from a larger population of possible raters. Example: one set of raters may assess a subgroup of subjects in one center and another set of raters may assess a subgroup of subjects in another center, and hence, 1-way random-effects model should be used in this case.
#### Two-Way Random-Effects Model
If we randomly select our raters from a larger population of raters with similar characteristics, 2-way random-effects model is the model of choice. In other words, we choose 2-way random-effects model if we plan to generalize our reliability results to any raters who possess the same characteristics as the selected raters in the reliability study.

#### Two-Way Mixed-Effects Model
We should use the 2-way mixed-effects model if the selected raters are the only raters of interest.With this model, the results only represent the reliability of the specific raters involved in the reliability experiment. They cannot be generalized to other raters even if those raters have similar characteristics as the selected raters in the reliability experiment. As a result, 2-way mixed-effects model is less commonly used in interrater reliability analysis.

### “Type” Selection
#### mean of k raters
If we plan to use the mean value of 3 raters as an assessment basis, the experimental design of the reliability study should involve 3 raters, and the “mean of k raters” type should be selected.
#### single rater”
Conversely, if we plan to use the measurement from a single rater as the basis of the actual measurement, “single rater” type should be selected even though the reliability experiment involves 2 or more raters.

### “Definition” Selection
For both 2-way random- and 2-way mixed-effects models, there are 2 ICC definitions: “**absolute agreement**” and “**consistency**.” Selection of the ICC definition depends on whether we consider absolute agreement or consistency between raters to be more important.

**Absolute agreement** concerns if different raters assign the same score to the same subject.

**Consistency** definition concerns if raters’ scores to the same group of subjects are correlated in an additive manner.

Example:
Consider an interrater reliability study of 2 raters as an example. In this case, *consistency* definition concerns the degree to which one rater’s score (y) can be equated to another rater’s score (x) plus a systematic error (c) (ie, y = x + c), whereas *absolute agreement* concerns about the extent to which y equals x.

### Selection flow chart
![](/images/blog/flowchart.png)

### Characteristic
>* 1.If the data sets are identical, all ICC estimates will equal to 1.
>* 2.Generally speaking, ICC of the “mean of k raters” type is larger than the corresponding “single rater” type.
>* 3.The “absolute agreement” definition generally gives a smaller ICC estimate than the “consistency” definition.
>* 4.One-way random-effects model generally gives a smaller ICC estimate than the 2-way models.
>* 5.For the same ICC definition (eg absolute agreement), ICC estimates of both the 2-way random- and mixed-effects models are the same because they use the same formula to calculate the ICC


### ICC Interpretation
We have to understand that there are no standard values for acceptable reliability using ICC.

As a rule of thumb, researchers should try to obtain at least 30 heterogeneous samples and involve at least 3 raters whenever possible when conducting a reliability study.

>* <0.5 poor reliability,
>* 0.5~0.75 moderate reliability,
>* 0.75~0.9 good reliability,
>* \>0.9 excellent reliability.

Reference:A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research
